\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}

\geometry{margin=1in}

% Lean code styling
\lstdefinelanguage{Lean}{
  keywords={def, theorem, lemma, example, import, open, do, let, if, then, else, for, in, return, 
  by, run_tac, TacticM, Unit, Prop, Type},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={getMainGoal, withMainContext, getLCtx, inferType, isDefEq, assign, throwError},
  ndkeywordstyle=\color{purple},
  identifierstyle=\color{black},
  sensitive=true,
  comment=[l]{--},
  morecomment=[s]{/-}{-/},
  commentstyle=\color{gray}\itshape,
  stringstyle=\color{orange},
  morestring=[b]",
}

\lstset{
  language=Lean,
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny\color{gray},
  tabsize=2,
}

\title{Automated Tactic Generation for Lean 4 \\
  \large From Informal Specifications to Metaprogramming Code}

\author{
  % Authors here
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present a pipeline for automatically generating Lean 4 tactics 
from informal natural language specifications. Our system leverages large 
language models (LLMs) to translate high-level descriptions of desired proof automation 
into working metaprogramming code. A key innovation is our specification-first 
approach: before implementation, the LLM must produce a formal mathematical characterization of 
the tactic's semantics, which is then embedded in the generated code and used 
to guide test generation. We employ a two-phase test generation approach 
- first generating a systematic algorithm for test case enumeration, 
then instantiating specific tests ordered by difficulty. 
The pipeline includes an iterative repair mechanism that uses compiler 
feedback to fix errors while preserving the specification. 
We describe the architecture, design choices, and evaluate 
the system on propositional logic tactics.
\end{abstract}

\section{Introduction}

Interactive theorem provers like Lean 4 have become increasingly 
important tools for formalizing mathematics and verifying software. 
A key feature of these systems is their extensibility through \emph{tactics}
---procedures that automate proof construction. However, 
writing tactics requires expertise in both the domain (e.g., mathematics) and metaprogramming, 
creating a barrier for many users.

This paper presents a system that bridges this gap by automatically 
generating Lean 4 tactics from informal natural language descriptions. 
Given a specification like ``create a tactic that proves implications by introducing 
hypotheses and searching the context,'' our system produces working metaprogramming 
code that can be used in proofs.

\subsection{Contributions}

\begin{itemize}
  \item A modular pipeline architecture for LLM-based tactic generation
  \item Mathematical specification extraction: forcing the LLM to formalize 
  the tactic's semantics before implementation
  \item A two-phase test generation approach: algorithm design followed by test instantiation
  \item Specification embedding in generated code for traceability and documentation
  \item An iterative repair mechanism using Lean compiler feedback
  \item Evaluation on propositional logic tactic generation
\end{itemize}

\section{Background}

\subsection{Lean 4 and Metaprogramming}

Lean 4 is a functional programming language and interactive theorem prover. 
Unlike its predecessor Lean 3, Lean 4 provides a unified metaprogramming 
framework where tactics are written in Lean itself.

A tactic in Lean 4 operates in the \texttt{TacticM} monad, which provides 
access to the proof state. The key components are:

\begin{itemize}
  \item \textbf{Goals}: Represented as metavariables (\texttt{MVarId}) that need to be assigned proof terms
  \item \textbf{Local context}: The hypotheses available in the current proof state
  \item \textbf{Expressions}: The \texttt{Expr} type represents Lean terms, types, and proofs
\end{itemize}

\begin{lstlisting}[caption={A simple tactic that searches the local context}]
def assumption_tactic : TacticM Unit :=
  withMainContext do
    let goal <- getMainGoal
    let ctx <- getLCtx
    for decl in ctx do
      try
        goal.assignIfDefEq (Expr.fvar decl.fvarId)
        return
      catch _ => pure ()
    throwError "No matching assumption found"
\end{lstlisting}

\subsection{Large Language Models for Code Generation}

Recent advances in large language models have demonstrated remarkable 
capabilities in code generation. Models like GPT-4 and Claude can 
generate syntactically correct code in many programming languages 
when given natural language descriptions.

However, generating \emph{correct} code for specialized domains 
like theorem prover metaprogramming remains challenging due to:

\begin{itemize}
  \item Limited training data for niche languages
  \item Complex type systems and semantic constraints
  \item The need for domain-specific knowledge
\end{itemize}

\section{System Architecture}

Our pipeline consists of six main stages: analysis, 
test algorithm generation, tactic generation, test generation, validation, and repair.

\subsection{Overview}

\begin{figure}[h]
\centering
\begin{verbatim}
+-------------------+
| Informal Request  |
+--------+----------+
         |
         v
+-------------------+
| 1. Analyze        | --> Structured specification
+--------+----------+
         |
         v
+-------------------+
| 2. Test Algorithm | --> Test generation strategy
+--------+----------+
         |
         v
+-------------------+
| 3. Generate Tactic| --> Lean 4 metaprogramming code
+--------+----------+
         |
         v
+-------------------+
| 4. Generate Tests | --> N test theorems (configurable)
+--------+----------+
         |
         v
+-------------------+
| 5. Validate       | --> Compile with Lake
+--------+----------+
         |
    +----+----+
    | Errors? |
    +----+----+
    Yes  |  No
    v    |
+-------+|
| Repair||---> Output validated tactic + tests
+---+---+|
    |    |
    +----+ (retry up to N rounds)
\end{verbatim}
\caption{Single-tactic pipeline architecture}
\end{figure}

\begin{figure}[h]
\centering
\begin{verbatim}
+----------------------+
| specifications.json  |  (multiple user specifications)
+----------+-----------+
           |
           v
+----------------------+
| 0. Split Spec        | --> Identify multiple tactics
+----------+-----------+
           |
    +------+------+
    |             |
    v             v
+--------+   +--------+
| Spec 1 |   | Spec N |  ... (one or more tactics)
+---+----+   +---+----+
    |             |
    v             v
+--------+   +--------+
|Pipeline|   |Pipeline|  (full single-tactic pipeline)
+---+----+   +---+----+
    |             |
    v             v
+----------------------+
| Aggregate Results    | --> JSON report
+----------------------+
\end{verbatim}
\caption{Batch processing architecture}
\end{figure}

\begin{figure}[h]
\centering
\begin{verbatim}
+------------------+     +------------------+
| existing.lean    |     | existing.spec.md |
+--------+---------+     +--------+---------+
         |                        |
         v                        v
+----------------------------------------+
| 1. Parse existing tests & specification |
+-------------------+--------------------+
                    |
                    v
+----------------------------------------+
| 2. Generate N additional tests          |
|    (non-duplicates, following algorithm)|
+-------------------+--------------------+
                    |
                    v
+----------------------------------------+
| 3. Append tests to file                 |
+-------------------+--------------------+
                    |
                    v
+----------------------------------------+
| 4. Validate & Repair                    |
|    (update tactic if tests fail)        |
+-------------------+--------------------+
                    |
                    v
+----------------------------------------+
| 5. Save updated file                    |
+----------------------------------------+
\end{verbatim}
\caption{Update pipeline architecture}
\end{figure}

\subsection{Analysis Stage}

The analysis stage extracts structured information from the informal 
request, beginning with a \emph{scope analysis} before producing a \emph{mathematical specification}:

\begin{itemize}
  \item \textbf{Tactic name}: A suitable identifier for the tactic
  \item \textbf{Scope Analysis}: Before committing to a specification, the LLM reasons about:
    \begin{itemize}
      \item The most general interpretation of the user's request
      \item Constraints imposed by Lean 4 metaprogramming (decidability, available APIs)
      \item Implementation complexity trade-offs
      \item The chosen scope, with justification
    \end{itemize}
  \item \textbf{Mathematical Specification}: A rigorous characterization including:
    \begin{itemize}
      \item The class of formulas/goals handled (using set-builder notation)
      \item The logical fragment (propositional, first-order, etc.)
      \item Precise provability conditions
    \end{itemize}
  \item \textbf{Algorithm}: Step-by-step description of the proof search procedure
  \item \textbf{Success criteria}: When the tactic succeeds vs.\ fails
  \item \textbf{Edge cases}: Potential failure modes to handle
\end{itemize}

The scope analysis is crucial: it forces the LLM to find the right balance 
between generality (handling a broad class of formulas) and feasibility 
(what can actually be implemented in Lean). For example, for an implication-selecting tactic:

\begin{quote}
\textbf{Scope Analysis}: The most general interpretation would handle arbitrary intuitionistic proofs, 
but this requires full proof search. A feasible scope is right-associative implication chains 
where the conclusion matches a premise---this is decidable and directly implementable 
using \texttt{intro} and hypothesis matching.

\textbf{Formula Class}: $\mathcal{F} = \{\varphi \mid \varphi \equiv 
P_1 \to P_2 \to \cdots \to P_n \to Q \text{ where } Q \in \{P_1, \ldots, P_n\}\}$

\textbf{Provability}: $\forall \varphi \in \mathcal{F}.\; \vdash \varphi \iff \exists i.\; P_i \equiv Q$
\end{quote}

This specification serves as the formal contract for the tactic and is preserved throughout the pipeline.

\subsection{Test Algorithm Generation Stage}

Before generating the tactic implementation, we first generate a \emph{test generation algorithm}. 
This is a key innovation of our approach: rather than generating test cases directly, we ask the LLM 
to design a systematic method for generating tests based on its understanding of the tactic's specification.

The test algorithm typically includes:
\begin{itemize}
  \item \textbf{Formula structure}: What shapes of formulas the tactic handles (e.g., implication chains, conjunctions)
  \item \textbf{Parameters}: Variables that can be adjusted (number of propositions, nesting depth)
  \item \textbf{Provability criteria}: Conditions that ensure generated theorems are actually provable by the tactic
  \item \textbf{Difficulty progression}: How to order tests from simple to complex
\end{itemize}

For example, for implicational tactics, the algorithm might specify enumerating formulas 
like $A \to B \to A$ where the conclusion matches one of the premises.

\subsection{Tactic Generation Stage}

The tactic generation stage produces Lean 4 code based on the structured specification. We provide the LLM with:

\begin{enumerate}
  \item The structured specification from the analysis stage
  \item A comprehensive tutorial on Lean 4 metaprogramming (based on \cite{tactic-guide})
  \item Example tactic implementations
  \item The required output format
\end{enumerate}

\subsubsection{Output Format}

Generated tactics follow a standardized format:

\begin{lstlisting}[caption={Required output format}]
import Lean
open Lean Meta Elab.Tactic

/-- Helper to run a TacticM function in a proof. -/
elab "run_tac" tac:ident : tactic => do
  let name := tac.getId
  let some (.defnInfo _) := (<- getEnv).find? name
    | throwError s!"unknown tactic function: {name}"
  let tacticFn <- unsafe evalConst (TacticM Unit) name
  tacticFn

/-- Documentation for the tactic. -/
def tactic_name : TacticM Unit := do
  -- Implementation
\end{lstlisting}

The \texttt{run\_tac} elaborator allows invoking \texttt{TacticM} functions directly in proofs, 
enabling immediate testing of generated tactics without requiring custom syntax declarations.

\subsubsection{Specification Embedding}

The mathematical specification from the analysis stage is embedded directly in the generated 
file as a module-level documentation comment:

\begin{lstlisting}[caption={Specification embedded in generated code}]
/-!
# Tactic: select_hypothesis

## Mathematical Specification
F = {phi | phi = P1 -> P2 -> ... -> Pn -> Q
     where Q in {P1, ..., Pn}}

## Algorithm
1. Parse goal as nested implications
2. Introduce all hypotheses
3. Find and apply matching hypothesis
-/
\end{lstlisting}

Additionally, a separate \texttt{.spec.md} file is saved alongside the \texttt{.lean} file, 
containing the original request, full analysis, and test generation algorithm. This serves as 
a permanent record and enables traceability from informal specification to implementation.

\subsection{Test Generation Stage}

Using the test algorithm from Stage 2, we generate a configurable number of test theorems. 
The tests are structured to provide comprehensive coverage:

\begin{itemize}
  \item \textbf{First third}: Simplest cases with minimal variables
  \item \textbf{Middle third}: Medium complexity with more variables and deeper nesting
  \item \textbf{Final third}: Edge cases and maximum complexity the tactic should handle
\end{itemize}

All tests use \texttt{run\_tac tactic\_name} as the proof, ensuring they compile if and 
only if the tactic succeeds. The number of tests is user-configurable (default: 10).

\subsection{Validation Stage}

Generated code is validated by compiling it with the Lean toolchain:

\begin{verbatim}
lake env lean <generated_file.lean>
\end{verbatim}

The validator parses compiler output to extract:
\begin{itemize}
  \item Errors with line numbers and messages
  \item Warnings (optionally treated as errors)
\end{itemize}

\subsection{Repair Stage}

When validation fails, the repair stage attempts to fix the code by:

\begin{enumerate}
  \item Providing the LLM with the original code and error messages
  \item Requesting a corrected version
  \item Re-validating the result
\end{enumerate}

This process repeats up to a configurable maximum number of rounds (default: 4).

\section{Design Choices}

\subsection{Model Abstraction}

The pipeline supports multiple LLM providers through an abstract interface:

\begin{lstlisting}[language=Python,caption={LLM abstraction layer}]
class LLMModel(ABC):
    @abstractmethod
    def generate(self, prompt: str,
                 system: Optional[str] = None) -> str:
        pass
\end{lstlisting}

This design allows easy switching between providers (OpenAI, Anthropic, OpenRouter) and facilitates
future extensions to local models. OpenRouter support enables access to a wide variety of models
(including Google Gemini, Meta LLaMA, Mistral, and others) through a unified API.

\subsection{Batch Processing and Multi-Tactic Specifications}

Real-world tactic requests often describe multiple related but distinct tactics in a single specification.
For example, a user might request ``a tactic that can compute polynomial equality, coefficient extraction,
and leading term.'' Rather than generating a monolithic tactic, our system:

\begin{enumerate}
  \item \textbf{Specification Splitting}: An LLM analyzes the user specification to identify distinct tactics.
  Each is extracted as a self-contained specification with its own name and description.
  \item \textbf{Independent Generation}: Each split specification goes through the full pipeline independently,
  producing separate tactic files.
  \item \textbf{Aggregated Reporting}: Results are collected and reported together, showing success/failure
  per tactic and per original specification.
\end{enumerate}

This approach has several advantages:
\begin{itemize}
  \item \textbf{Modularity}: Each tactic is self-contained and can be used independently
  \item \textbf{Targeted repair}: If one tactic fails, others are unaffected
  \item \textbf{Clearer specifications}: Smaller, focused specifications lead to better-defined tactics
\end{itemize}

The batch mode also supports processing multiple specifications from a JSON configuration file,
enabling systematic generation of tactic suites from curated specification collections.

\subsection{Incremental Update Pipeline}

Once a tactic is generated, it may need to be improved to handle additional cases. Our \emph{update pipeline}
enables incremental enhancement without regenerating from scratch:

\begin{enumerate}
  \item \textbf{Specification Reuse}: The original specification and test generation algorithm
  (stored in the \texttt{.spec.md} file) guide the generation of new tests
  \item \textbf{Non-duplicate Tests}: The LLM is shown existing tests and instructed to generate
  novel cases covering different parameters, edge cases, or complexity levels
  \item \textbf{Targeted Repair}: When new tests fail, the repair prompt focuses on extending
  the tactic's capabilities rather than rewriting it entirely
\end{enumerate}

This incremental approach has several benefits:
\begin{itemize}
  \item \textbf{Preserves working functionality}: Existing tests continue to pass
  \item \textbf{Gradual capability expansion}: The tactic grows to handle more cases over iterations
  \item \textbf{Efficient use of LLM calls}: Only new tests and targeted repairs are generated
  \item \textbf{Convergence toward specification}: Repeated updates can help the tactic
  asymptotically cover its full specification
\end{itemize}

\subsection{Mathlib Configuration}

The system supports an optional Mathlib dependency through a configuration flag. When enabled:
\begin{itemize}
  \item Generated tactics can import Mathlib modules
  \item The prompt includes Mathlib-specific guidance
  \item More sophisticated mathematical tactics become possible
\end{itemize}

When disabled, tactics are restricted to core Lean and the Batteries library, ensuring minimal dependencies.

\subsection{Tutorial Integration}

We integrate content from the Lean Tactic Programming Guide \cite{tactic-guide} directly 
into the generation prompt. This provides:
\begin{itemize}
  \item Concrete examples of working tactics
  \item Common metaprogramming patterns
  \item Idiomatic Lean 4 style
\end{itemize}

\subsection{Specification-First Design}

A crucial design choice is requiring a formal mathematical specification \emph{before} implementation. This serves multiple purposes:

\begin{itemize}
  \item \textbf{Semantic clarity}: Forces the LLM to precisely characterize what the tactic proves, using set-builder notation and provability conditions
  \item \textbf{Implementation guidance}: The specification constrains the implementation, reducing the space of possible (incorrect) solutions
  \item \textbf{Test generation}: The specification directly informs what test cases should be generated
  \item \textbf{Repair consistency}: During repair, the specification ensures fixes maintain semantic correctness
  \item \textbf{Documentation}: The embedded specification serves as authoritative documentation
\end{itemize}

This approach mirrors formal methods practice, where specifications precede implementations. The specification is passed to all subsequent stages (test algorithm, tactic generation, test generation, repair), ensuring consistency throughout the pipeline.

\subsection{Two-Phase Test Generation}

A key design choice is separating test generation into two phases: algorithm design and test instantiation. This approach has several advantages:

\begin{itemize}
  \item \textbf{Systematic coverage}: The algorithm ensures tests cover the space of valid inputs methodically
  \item \textbf{Provability guarantee}: The algorithm explicitly encodes what makes a formula provable by the tactic
  \item \textbf{Difficulty ordering}: Tests progress from trivial to challenging, aiding debugging
  \item \textbf{Configurability}: Users can request any number of tests without changing the generation logic
\end{itemize}

This is more robust than generating tests directly, which often produces duplicates or invalid cases.

\section{Evaluation}

\subsection{Experimental Setup}

% TODO: Add experimental details

\subsection{Results}

% TODO: Add results

\subsection{Case Study: Propositional Logic Tactics}

We evaluated the system on generating tactics for intuitionistic propositional logic. Example specification:

\begin{quote}
``Create a tactic that proves implications of the form $A \to B \to A$ or $A \to B \to B$ by introducing hypotheses and selecting the appropriate one.''
\end{quote}

The system first generated a test algorithm specifying:
\begin{itemize}
  \item Formulas of the form $P_1 \to P_2 \to \cdots \to P_n \to P_i$ where $P_i$ is one of the premises
  \item Variable count ranging from 1 to 6 propositions
  \item Ordering by number of premises (simple to complex)
\end{itemize}

The generated tactic uses recursive helper functions to collect all premises and find a matching hypothesis:

\begin{lstlisting}[caption={Generated propositional logic tactic (excerpt)}]
/-- Recursively collect all premise types -/
partial def collectPremises (type : Expr)
    : MetaM (Array Expr * Expr) := do
  let type <- whnf type
  match type with
  | .forallE _ domain body _ =>
    let (rest, conclusion) <- collectPremises body
    return (#[domain] ++ rest, conclusion)
  | _ => return (#[], type)

/-- Main tactic: select matching hypothesis -/
def select_hyp : TacticM Unit := do
  withMainContext do
    let goal <- getMainGoal
    let (premises, conclusion) <- collectPremises
                                    (<- goal.getType)
    -- Find which premise matches conclusion
    for i in [:premises.size] do
      if <- isDefEq conclusion premises[i]! then
        introAndSelect premises.size i
        return
    throwError "No matching hypothesis"
\end{lstlisting}

The system generated 15 test theorems covering cases from $A \to A$ (simplest) to $P \to Q \to R \to S \to T \to U \to U$ (most complex), all compiling successfully after 2 repair rounds.

\section{Related Work}

\subsection{LLM-Based Theorem Proving}

% TODO: Discuss related work on LLMs for theorem proving

\subsection{Tactic Learning}

% TODO: Discuss tactic learning approaches

\subsection{Program Synthesis}

% TODO: Discuss program synthesis

\section{Limitations and Future Work}

\subsection{Current Limitations}

\begin{itemize}
  \item Generated tactics may be overly specific to the examples in the specification
  \item Complex tactics requiring deep mathematical insight remain challenging
  \item The repair mechanism has limited ability to fix fundamental design errors
\end{itemize}

\subsection{Future Directions}

\begin{itemize}
  \item Integration with proof search to guide tactic design
  \item Learning from existing Mathlib tactics
  \item Interactive refinement with user feedback
  \item Formal verification of generated tactics
\end{itemize}

\section{Conclusion}

We presented a pipeline for automatically generating Lean 4 tactics from informal specifications. The system combines LLM-based code generation with compiler-driven repair to produce working metaprogramming code. Our modular architecture supports multiple LLM providers and configurable dependencies, making it adaptable to different use cases.

% TODO: Summarize key findings

\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{tactic-guide}
Miroslav Olšák.
\textit{Lean Tactic Programming Guide}.
\url{https://github.com/mirefek/lean-tactic-programming-guide}, 2024.

\bibitem{lean4}
Leonardo de Moura and Sebastian Ullrich.
\textit{The Lean 4 Theorem Prover and Programming Language}.
CADE 2021.

\bibitem{mathlib}
The mathlib Community.
\textit{The Lean Mathematical Library}.
CPP 2020.

% TODO: Add more references

\end{thebibliography}

\end{document}
